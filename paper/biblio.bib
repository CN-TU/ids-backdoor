
@inproceedings{eldan_power_2016,
	title = {The power of depth for feedforward neural networks},
	booktitle = {Conference on learning theory},
	author = {Eldan, Ronen and Shamir, Ohad},
	year = {2016},
	pages = {907--940}
}

@inproceedings{bachl_sparseids_2020,
	title = {{SparseIDS}: {Learning} {Packet} {Sampling} with {Reinforcement} {Learning}},
	shorttitle = {{SparseIDS}},
	abstract = {Recurrent Neural Networks (RNNs) have been shown to be valuable for constructing Intrusion Detection Systems (IDSs) for network data. They allow determining if a flow is malicious or not already before it is over, making it possible to take action immediately. However, considering the large number of packets that has to be inspected, for example in cloud/fog and edge computing, the question of computational efficiency arises. We show that by using a novel Reinforcement Learning (RL)-based approach called SparseIDS, we can reduce the number of consumed packets by more than three fourths while keeping classification accuracy high. To minimize the computational expenses of the RL-based sampling we show that a shared neural network can be used for both the classifier and the RL logic. Thus, no additional resources are consumed by the sampling in deployment. Comparing to various other sampling techniques, SparseIDS consistently achieves higher classification accuracy by learning to sample only relevant packets. A major novelty of our RL-based approach is that it can not only skip up to a predefined maximum number of samples like other approaches proposed in the domain of Natural Language Processing but can even skip arbitrarily many packets in one step. This enables saving even more computational resources for long sequences. Inspecting SparseIDS's behavior of choosing packets shows that it adopts different sampling strategies for different attack types and network flows. Finally we build an automatic steering mechanism that can guide SparseIDS in deployment to achieve a desired level of sparsity.},
	booktitle = {2020 {IEEE} {Conference} on {Communications} and {Network} {Security} ({CNS})},
	publisher = {IEEE},
	author = {Bachl, Maximilian and Meghdouri, Fares and Fabini, Joachim and Zseby, Tanja},
	year = {2020},
	note = {arXiv: 2002.03872},
	keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
}

@inproceedings{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2019}
}

@inproceedings{yu_learning_2017,
	address = {Vancouver, Canada},
	title = {Learning to {Skim} {Text}},
	doi = {10.18653/v1/P17-1172},
	abstract = {Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.},
	urldate = {2020-02-09},
	booktitle = {{ACL} 2017},
	publisher = {ACL},
	author = {Yu, Adams Wei and Lee, Hongrae and Le, Quoc},
	month = jul,
	year = {2017},
	pages = {1880--1890},
}

@inproceedings{seo_neural_2018,
	title = {Neural {Speed} {Reading} via {Skim}-{RNN}},
	abstract = {Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.},
	urldate = {2020-01-21},
	booktitle = {{ICLR}},
	author = {Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{shen_reasonet_2017,
	title = {Reasonet: {Learning} to stop reading in machine comprehension},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
	year = {2017},
	pages = {1047--1055}
}

@article{dechant_predicting_2019,
	title = {Predicting the accuracy of neural networks from final and intermediate layer outputs},
	author = {DeChant, Chad and Han, Seungwook and Lipson, Hod},
	year = {2019}
}

@article{jernite_variable_2017,
	title = {Variable {Computation} in {Recurrent} {Neural} {Networks}},
	abstract = {Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.},
	urldate = {2020-07-20},
	journal = {arXiv:1611.06188 [cs, stat]},
	author = {Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.06188},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
}

@article{graves_adaptive_2017,
	title = {Adaptive {Computation} {Time} for {Recurrent} {Neural} {Networks}},
	abstract = {This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and diﬀerentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1603.08983 [cs]},
	author = {Graves, Alex},
	month = feb,
	year = {2017},
	note = {arXiv: 1603.08983},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{schmidhuber_self-delimiting_2012,
	title = {Self-{Delimiting} {Neural} {Networks}},
	abstract = {Self-delimiting (SLIM) programs are a central concept of theoretical computer science, particularly algorithmic information \& probability theory, and asymptotically optimal program search (AOPS). To apply AOPS to (possibly recurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical SLIM NN have threshold activation functions. During a computational episode, activations are spreading from input neurons through the SLIM NN until the computation activates a special halt neuron. Weights of the NN's used connections define its program. Halting programs form a prefix code. The reset of the initial NN state does not cost more than the latest program execution. Since prefixes of SLIM programs influence their suffixes (weight changes occurring early in an episode influence which weights are considered later), SLIM NN learning algorithms (LAs) should execute weight changes online during activation spreading. This can be achieved by applying AOPS to growing SLIM NNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly classifying many different patterns, or solving many different robot control tasks, each connection keeps a list of tasks it is used for. The lists may be efficiently updated during training. To evaluate the overall effect of currently tested weight changes, a SLIM NN LA needs to re-test performance only on the efficiently computable union of tasks potentially affected by the current weight changes. Future SLIM NNs will be implemented on 3-dimensional brain-like multi-processor hardware. Their LAs will minimize task-specific total wire length of used connections, to encourage efficient solutions of subtasks by subsets of neurons that are physically close. The novel class of SLIM NN LAs is currently being probed in ongoing experiments to be reported in separate papers.},
	urldate = {2020-07-20},
	journal = {arXiv:1210.0118 [cs]},
	author = {Schmidhuber, Juergen},
	month = sep,
	year = {2012},
	note = {arXiv: 1210.0118},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{bolukbasi_adaptive_2017,
	title = {Adaptive {Neural} {Networks} for {Efficient} {Inference}},
	abstract = {We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal ({\textless}1\%) loss of top5 accuracy.},
	urldate = {2020-07-20},
	journal = {arXiv:1702.07811 [cs, stat]},
	author = {Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
	month = sep,
	year = {2017},
	note = {arXiv: 1702.07811},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{leroux_cascading_2017,
	title = {The cascading neural network: building the {Internet} of {Smart} {Things}},
	volume = {52},
	issn = {0219-1377, 0219-3116},
	shorttitle = {The cascading neural network},
	doi = {10.1007/s10115-017-1029-1},
	abstract = {Most of the research on deep neural networks so far has been focused on obtaining higher accuracy levels by building increasingly large and deep architectures. Training and evaluating these models is only feasible when large amounts of resources such as processing power and memory are available. Typical applications that could beneﬁt from these models are, however, executed on resource-constrained devices. Mobile devices such as smartphones already use deep learning techniques, but they often have to perform all processing on a remote cloud. We propose a new architecture called a cascading network that is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network trafﬁc to a minimum. The network begins processing on the constrained device, and only relies on the remote part when the local part does not provide an accurate enough result. The cascading network allows for an early-stopping mechanism during the recall phase of the network. We evaluated our approach in an Internet of Things context where a deep neural network adds intelligence to a large amount of heterogeneous connected devices. This technique enables a whole variety of autonomous systems where sensors, actuators and computing nodes can work together. We show that the cascading architecture allows for a substantial improvement in evaluation speed on constrained devices while the loss in accuracy is kept to a minimum.},
	language = {en},
	number = {3},
	urldate = {2020-07-21},
	journal = {Knowledge and Information Systems},
	author = {Leroux, Sam and Bohez, Steven and De Coninck, Elias and Verbelen, Tim and Vankeirsbilck, Bert and Simoens, Pieter and Dhoedt, Bart},
	month = sep,
	year = {2017},
	pages = {791--814},
}

@incollection{wang_efficient_2015,
	title = {Efficient {Learning} by {Directed} {Acyclic} {Graph} {For} {Resource} {Constrained} {Prediction}},
	urldate = {2020-07-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Joseph and Trapeznikov, Kirill and Saligrama, Venkatesh},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2152--2160},
}

@article{viola_robust_2004,
	title = {Robust real-time face detection},
	volume = {57},
	number = {2},
	journal = {International journal of computer vision},
	author = {Viola, Paul and Jones, Michael J},
	year = {2004},
	note = {Publisher: Springer},
	pages = {137--154}
}

@incollection{lefakis_joint_2010,
	title = {Joint {Cascade} {Optimization} {Using} {A} {Product} {Of} {Boosted} {Classifiers}},
	urldate = {2020-07-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Lefakis, Leonidas and Fleuret, Francois},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {1315--1323},
}

@article{xu_classifier_2014,
	title = {Classifier {Cascades} and {Trees} for {Minimizing} {Feature} {Evaluation} {Cost}},
	volume = {15},
	issn = {1533-7928},
	number = {62},
	urldate = {2020-07-21},
	journal = {Journal of Machine Learning Research},
	author = {Xu, Zhixiang (Eddie) and Kusner, Matt J. and Weinberger, Kilian Q. and Chen, Minmin and Chapelle, Olivier},
	year = {2014},
	pages = {2113--2144},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-07-21},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{moustafa_unsw-nb15_2015,
	title = {{UNSW}-{NB15}: a comprehensive data set for network intrusion detection systems ({UNSW}-{NB15} network data set)},
	shorttitle = {{UNSW}-{NB15}},
	doi = {10.1109/MilCIS.2015.7348942},
	abstract = {One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSWNB15 data set. This data set is available for research purposes and can be accessed from the link.},
	booktitle = {2015 {Military} {Communications} and {Information} {Systems} {Conference} ({MilCIS})},
	author = {Moustafa, Nour and Slay, Jill},
	month = nov,
	year = {2015},
	keywords = {telecommunication traffic, Feature extraction, computer network security, IP networks, Servers, Benchmark testing, Data models, low footprint attacks, network intrusion detection systems, network traffic, NIDS, pcap files, Telecommunication traffic, testbed, Training, UNSW-NB15 data set, UNSW-NB15 network data set},
	pages = {1--6},
}

@inproceedings{sharafaldin_toward_2018,
	title = {Toward generating a new intrusion detection dataset and intrusion traffic characterization.},
	booktitle = {{ICISSP}},
	author = {Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A},
	year = {2018},
	pages = {108--116}
}

@article{williams_preliminary_2006,
	title = {A preliminary performance comparison of five machine learning algorithms for practical {IP} traffic flow classification},
	volume = {36},
	issn = {0146-4833},
	doi = {10.1145/1163593.1163596},
	abstract = {The identification of network applications through observation of associated packet traffic flows is vital to the areas of network management and surveillance. Currently popular methods such as port number and payload-based identification exhibit a number of shortfalls. An alternative is to use machine learning (ML) techniques and identify network applications based on per-flow statistics, derived from payload-independent features such as packet length and inter-arrival time distributions. The performance impact of feature set reduction, using Consistency-based and Correlation-based feature selection, is demonstrated on Naïve Bayes, C4.5, Bayesian Network and Naïve Bayes Tree algorithms. We then show that it is useful to differentiate algorithms based on computational performance rather than classification accuracy alone, as although classification accuracy between the algorithms is similar, computational performance can differ significantly.},
	number = {5},
	urldate = {2020-07-21},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {Williams, Nigel and Zander, Sebastian and Armitage, Grenville},
	month = oct,
	year = {2006},
	keywords = {machine learning, traffic classification},
	pages = {5--16},
}

@misc{noauthor_sigmoid_2020,
	title = {Sigmoid function},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Sigmoid_function&oldid=964386757},
	abstract = {A sigmoid function is a mathematical function having a characteristic "S"-shaped curve or sigmoid curve.  A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:

  
    
      
        S
        (
        x
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  x
                
              
            
          
        
        =
        
          
            
              e
              
                x
              
            
            
              
                e
                
                  x
                
              
              +
              1
            
          
        
        .
      
    
    \{{\textbackslash}displaystyle S(x)=\{{\textbackslash}frac \{1\}\{1+e{\textasciicircum}\{-x\}\}\}=\{{\textbackslash}frac \{e{\textasciicircum}\{x\}\}\{e{\textasciicircum}\{x\}+1\}\}.\}
  Other standard sigmoid functions are given in the Examples section.
Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. Sigmoid functions most often show a return value (y axis) in the range 0 to 1. Another commonly used range is from −1 to 1.
A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic density, the normal density, and Student's t probability density functions.  The sigmoid function is invertible, and its inverse is the logit function.},
	language = {en},
	urldate = {2020-07-21},
	journal = {Wikipedia},
	month = jun,
	year = {2020},
	note = {Page Version ID: 964386757},
}

@inproceedings{leroux_resource-constrained_2015,
	title = {Resource-constrained classification using a cascade of neural network layers},
	doi = {10.1109/IJCNN.2015.7280601},
	abstract = {Deep neural networks are the state of the art technique for a wide variety of classification problems. Although deeper networks are able to make more accurate classifications, the value brought by an additional hidden layer diminishes rapidly. Even shallow networks are able to achieve relatively good results on various classification problems. Only for a small subset of the samples do the deeper layers make a significant difference. We describe an architecture in which only the samples that can not be classified with a sufficient confidence by a shallow network have to be processed by the deeper layers. Instead of training a network with one output layer at the end of the network, we train several output layers, one for each hidden layer. When an output layer is sufficiently confident in this result, we stop propagating at this layer and the deeper layers need not be evaluated. The choice of a threshold confidence value allows us to trade-off accuracy and speed.},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Leroux, Sam and Bohez, Steven and Verbelen, Tim and Vankeirsbilck, Bert and Simoens, Pieter and Dhoedt, Bart},
	month = jul,
	year = {2015},
	note = {ISSN: 2161-4407},
	keywords = {Training, Neurons, deep neural network, pattern classification, Biological neural networks, neural net architecture, resource allocation, neural network architecture, resource-constrained classification, threshold confidence value},
	pages = {1--7},
}

@misc{maas_rectifier_2013,
	title = {Rectifier {Nonlinearities} {Improve} {Neural} {Network} {Acoustic} {Models}},
	abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
	language = {en},
	urldate = {2020-07-25},
	author = {Maas, Andrew L.},
	year = {2013}
}

@misc{noauthor_softmax_2020,
	title = {Softmax function},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=968517329},
	abstract = {In mathematics, the softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval 
  
    
      
        (
        0
        ,
        1
        )
      
    
    \{{\textbackslash}displaystyle (0,1)\}
  , and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.
The standard (unit) softmax function 
  
    
      
        σ
        :
        
          
            R
          
          
            K
          
        
        →
        
          
            R
          
          
            K
          
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma :{\textbackslash}mathbb \{R\} {\textasciicircum}\{K\}{\textbackslash}to {\textbackslash}mathbb \{R\} {\textasciicircum}\{K\}\}
  is defined by the formula

  
    
      
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
        
           for 
        
        i
        =
        1
        ,
        …
        ,
        K
        
           and 
        
        
          z
        
        =
        (
        
          z
          
            1
          
        
        ,
        …
        ,
        
          z
          
            K
          
        
        )
        ∈
        
          
            R
          
          
            K
          
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{z\_\{j\}\}\}\}\{{\textbackslash}text\{ for \}\}i=1,{\textbackslash}dotsc ,K\{{\textbackslash}text\{ and \}\}{\textbackslash}mathbf \{z\} =(z\_\{1\},{\textbackslash}dotsc ,z\_\{K\}){\textbackslash}in {\textbackslash}mathbb \{R\} {\textasciicircum}\{K\}\}
  In words: we apply the standard exponential function to each element 
  
    
      
        
          z
          
            i
          
        
      
    
    \{{\textbackslash}displaystyle z\_\{i\}\}
   of the input vector 
  
    
      
        
          z
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbf \{z\} \}
   and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector 
  
    
      
        σ
        (
        
          z
        
        )
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\}
   is 1.
Instead of e, a different base b {\textgreater} 0 can be used; choosing a larger value of b will create a probability distribution that is more concentrated around the positions of the largest input values. Writing 
  
    
      
        b
        =
        
          e
          
            β
          
        
      
    
    \{{\textbackslash}displaystyle b=e{\textasciicircum}\{{\textbackslash}beta \}\}
   or 
  
    
      
        b
        =
        
          e
          
            −
            β
          
        
      
    
    \{{\textbackslash}displaystyle b=e{\textasciicircum}\{-{\textbackslash}beta \}\}
   (for real β) yields the expressions:

  
    
      
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                β
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  β
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
        
           or 
        
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                −
                β
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  −
                  β
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
        
           for 
        
        i
        =
        1
        ,
        …
        ,
        K
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{{\textbackslash}beta z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{{\textbackslash}beta z\_\{j\}\}\}\}\{{\textbackslash}text\{ or \}\}{\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{-{\textbackslash}beta z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{-{\textbackslash}beta z\_\{j\}\}\}\}\{{\textbackslash}text\{ for \}\}i=1,{\textbackslash}dotsc ,K\}
  .In some fields, the base is fixed, corresponding to a fixed scale, while in others the parameter β is varied.},
	language = {en},
	urldate = {2020-07-23},
	journal = {Wikipedia},
	month = jul,
	year = {2020},
	note = {Page Version ID: 968517329}
}

@article{ferrag_deep_2020,
	title = {Deep learning for cyber security intrusion detection: {Approaches}, datasets, and comparative study},
	volume = {50},
	issn = {2214-2126},
	shorttitle = {Deep learning for cyber security intrusion detection},
	doi = {10.1016/j.jisa.2019.102419},
	abstract = {In this paper, we present a survey of deep learning approaches for cyber security intrusion detection, the datasets used, and a comparative study. Specifically, we provide a review of intrusion detection systems based on deep learning approaches. The dataset plays an important role in intrusion detection, therefore we describe 35 well-known cyber datasets and provide a classification of these datasets into seven categories; namely, network traffic-based dataset, electrical network-based dataset, internet traffic-based dataset, virtual private network-based dataset, android apps-based dataset, IoT traffic-based dataset, and internet-connected devices-based dataset. We analyze seven deep learning models including recurrent neural networks, deep neural networks, restricted Boltzmann machines, deep belief networks, convolutional neural networks, deep Boltzmann machines, and deep autoencoders. For each model, we study the performance in two categories of classification (binary and multiclass) under two new real traffic datasets, namely, the CSE-CIC-IDS2018 dataset and the Bot-IoT dataset. In addition, we use the most important performance indicators, namely, accuracy, false alarm rate, and detection rate for evaluating the efficiency of several methods.},
	language = {en},
	urldate = {2020-07-22},
	journal = {Journal of Information Security and Applications},
	author = {Ferrag, Mohamed Amine and Maglaras, Leandros and Moschoyiannis, Sotiris and Janicke, Helge},
	month = feb,
	year = {2020},
	keywords = {Machine learning, Deep learning, Cyber security, Intrusion detection,},
	pages = {102419}
}

@article{berman_survey_2019,
	title = {A {Survey} of {Deep} {Learning} {Methods} for {Cyber} {Security}},
	volume = {10},
	doi = {10.3390/info10040122},
	abstract = {This survey paper describes a literature review of deep learning (DL) methods for cyber security applications. A short tutorial-style description of each DL method is provided, including deep autoencoders, restricted Boltzmann machines, recurrent neural networks, generative adversarial networks, and several others. Then we discuss how each of the DL methods is used for security applications. We cover a broad array of attack types including malware, spam, insider threats, network intrusions, false data injection, and malicious domain names used by botnets.},
	language = {en},
	number = {4},
	urldate = {2020-07-22},
	journal = {Information},
	author = {Berman, Daniel S. and Buczak, Anna L. and Chavis, Jeffrey S. and Corbett, Cherita L.},
	month = apr,
	year = {2019},
	keywords = {deep learning, deep autoencoders, deep neural networks, convolutional neural networks, cyber analytics, deep belief networks, restricted Boltzmann machines},
	pages = {122}
}

@inproceedings{vigneswaran_evaluating_2018,
	title = {Evaluating {Shallow} and {Deep} {Neural} {Networks} for {Network} {Intrusion} {Detection} {Systems} in {Cyber} {Security}},
	doi = {10.1109/ICCCNT.2018.8494096},
	abstract = {Intrusion detection system (IDS) has become an essential layer in all the latest ICT system due to an urge towards cyber safety in the day-to-day world. Reasons including uncertainty in finding the types of attacks and increased the complexity of advanced cyber attacks, IDS calls for the need of integration of Deep Neural Networks (DNNs). In this paper, DNNs have been utilized to predict the attacks on Network Intrusion Detection System (N-IDS). A DNN with 0.1 rate of learning is applied and is run for 1000 number of epochs and KDDCup-`99' dataset has been used for training and benchmarking the network. For comparison purposes, the training is done on the same dataset with several other classical machine learning algorithms and DNN of layers ranging from 1 to 5. The results were compared and concluded that a DNN of 3 layers has superior performance over all the other classical machine learning algorithms.},
	booktitle = {2018 9th {International} {Conference} on {Computing}, {Communication} and {Networking} {Technologies} ({ICCCNT})},
	author = {Vigneswaran, Rahul K. and Vinayakumar, R. and Soman, K.P. and Poornachandran, Prabaharan},
	month = jul,
	year = {2018},
	keywords = {machine learning, deep learning, learning (artificial intelligence), Machine learning, security of data, Intrusion detection, Training, neural nets, Biological neural networks, cyber security, Machine learning algorithms, Computer security, deep neural networks, DNN, advanced cyber attacks, classical machine learning algorithms, cyber safety, ICT system, N-IDS, network intrusion detection system},
	pages = {1--6}
}

@inproceedings{almseidin_evaluation_2017,
	title = {Evaluation of machine learning algorithms for intrusion detection system},
	doi = {10.1109/SISY.2017.8080566},
	abstract = {Intrusion detection system (IDS) is one of the implemented solutions against harmful attacks. Furthermore, attackers always keep changing their tools and techniques. However, implementing an accepted IDS system is also a challenging task. In this paper, several experiments have been performed and evaluated to assess various machine learning classifiers based on KDD intrusion dataset. It succeeded to compute several performance metrics in order to evaluate the selected classifiers. The focus was on false negative and false positive performance metrics in order to enhance the detection rate of the intrusion detection system. The implemented experiments demonstrated that the decision table classifier achieved the lowest value of false negative while the random forest classifier has achieved the highest average accuracy rate.},
	booktitle = {2017 {IEEE} 15th {International} {Symposium} on {Intelligent} {Systems} and {Informatics} ({SISY})},
	author = {Almseidin, Mohammad and Alzubi, Maen and Kovacs, Szilveszter and Alkasassbeh, Mouhammd},
	month = sep,
	year = {2017},
	note = {ISSN: 1949-0488},
	keywords = {learning (artificial intelligence), Prediction algorithms, Algorithm design and analysis, security of data, Intrusion detection, Testing, Neural networks, intrusion detection system, pattern classification, Machine learning algorithms, harmful attacks, IDS system, KDD intrusion dataset, machine learning classifiers, performance metrics, Vegetation},
	pages = {000277--000282}
}

@article{buczak_survey_2016,
	title = {A {Survey} of {Data} {Mining} and {Machine} {Learning} {Methods} for {Cyber} {Security} {Intrusion} {Detection}},
	volume = {18},
	issn = {1553-877X},
	doi = {10.1109/COMST.2015.2494502},
	abstract = {This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.},
	number = {2},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Buczak, Anna L. and Guven, Erhan},
	year = {2016},
	keywords = {computer network security, Computer security, cyber analytics, Cyber analytics, Cyber Analytics, cyber data sets, cyber security intrusion detection, data mining, Data mining, Data Mining, Data models, DM methods, IP networks, learning (artificial intelligence), machine learning, Machine Learning, machine learning methods, Measurement, ML-DM algorithms, ML-DM method, Ports (Computers), Protocols},
	pages = {1153--1176}
}
